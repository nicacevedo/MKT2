theme(plot.title = element_text(hjust = 0.5)) #centra el título en el gráfico. Lo ajusta en la posición horizontal (hjust = 0.5)
#R no siempre interpreta bien la naturaleza de las variables: Roof_Style y Heating_QC son factores, pero están como character(string)
#en train
train$Roof_Style=as.factor(train$Roof_Style)
train$Heating_QC=as.factor(train$Heating_QC)
#en test
test$Roof_Style=as.factor(test$Roof_Style)
test$Heating_QC=as.factor(test$Heating_QC)
library(gridExtra) #Para unir gráficas
g1 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size=1, alpha=0.4) + #"size" aumenta el tamaño de los puntos, "alpha" da transparencia
geom_smooth(se=FALSE) + #Agregamos un ajuste no lineal sobre los puntos. "se" integra errores estándares
xlab("Año de construcción")
g2 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size = 1, alpha = .4) +
geom_smooth(method = "lm", se = FALSE) + #Agregamos un ajuste lineal sobre los puntos.
scale_y_log10() + # "scale_y_log10" transforma el eje "y" a logaritmo.
xlab("Año de construcción")
grid.arrange(g1, g2, nrow = 1) #une las  gráficas.
ggplot(Casas) +
aes(x=Gr_Liv_Area, y=Sale_Price, col=Heating_QC)+ #Se agrega una dimensión de colores "col".
geom_point(size=1, alpha=0.4) +
geom_smooth(se=FALSE, method="lm") +
xlab("Espacio habitable")
ggplot(Casas) +
aes(x=Roof_Style, y=log(Sale_Price)) +
geom_boxplot(alpha=0.4, fill="black") #cambiamos el tipo de gráfico
train.lm <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC, #Fórmula
data = train, #Datos
method = "lm", #Algoritmo
trControl = trainControl(method = "cv", number = 10) #Method = cross validation, number=10 (k-fold)
)
test.lm  <- predict(train.lm , newdata=test) #Vector de datos predichos. Recibe una base de datos (newdata) y un modelo entrenado (train.lm)
error.lm <- test$Sale_Price-test.lm #Calcular los errores de predicción (dato real - dato estimado)
#Ejecutar MARS (Multivariate adaptive regression spline)
train.mars <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train,
method="earth", #MARS
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"), #Pre-procesa datos. "center" resta el promedio de las variables, "scale" las divide por la desviación estandar. Esto ayuda para el tratamiento de outliers.
tuneLength = 5 #Indica que pruebe diferentes valores por default para el parámetro principal
)
print(train.mars)
ggplot(train.mars)
test.mars  <- predict(train.mars, newdata=test) #Vector de datos predichos
error.mars <- test$Sale_Price-test.mars #(dato real - dato estimado)
### Ejecutar KNN
train.knn <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="knn",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.knn)
ggplot(train.knn)
test.knn  <- predict(train.knn, newdata=test)
error.knn <- test$Sale_Price-test.knn
### Ejecutar CART (Classification and Regression Trees)
train.cart <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rpart2",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.cart)
ggplot(train.cart)
test.cart  <- predict(train.cart, newdata=test)
error.cart <- test$Sale_Price-test.cart
### Ejecutar Random Forest
train.randomf <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rf",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.randomf)
ggplot(train.randomf)
test.randomf  <- predict(train.randomf, newdata=test)
error.randomf <- test$Sale_Price-test.randomf
sales.test <- data.frame(lm=test.lm, mars=unname(test.mars),  knn=test.knn,  cart=test.cart,  rf=test.randomf, sales=test$Sale_Price)
error.test <- data.frame(lm=error.lm, mars=unname(error.mars), knn=error.knn, cart=error.cart, rf=error.randomf)
summary(abs(error.test))
summary(error.test)
boxplot(abs(subset(error.test, select=-lm))); title(main="ML models", sub="Forecasting Absolute Errors")
boxplot(subset(error.test, select=-lm)); title(main="ML models", sub="Forecasting Errors")
View(Casas)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
getwd()
read.csv('Casas.csv')
read.csv('Casas.csv')
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
source('D:/Documents/Fcfm/Semestre 9/Marketing II/MKT2/Lab2/Lab2.R', encoding = 'UTF-8', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())				 #Limpia todos los objetos creados en R
graphics.off()			 #Limpia los gráficos
options(digits = 3)  #Dígitos después del punto para observar (décimas, centésimas,...)
set.seed(12345)      #Fijar semilla de aleatoriedad
# setwd("C:/Users/color/Downloads/Marketing/LAB2_IN5602") #Fijar directorio de preferencia
library(readr)     #Para leer CSV
library(glmnet)    #Para ajustar modelo lineal
library(corrplot)  #Para realizar correlogramas
library(dplyr)     #Para manipulación de datos
library(ggplot2)   #Para gráficos
library(caret)     #Para validar y entrenar los modelos
library(knitr)
Casas <- read.csv("Casas.csv")
kable(Casas[1:5,1:9]) #kable crea una tabla head(Casas) sirve para visualiar la data
# Construimos un vector que esta formado por números de filas aleatoriamente
index <- sample(1:nrow(Casas), size= nrow(Casas)*0.7)
# Base de entrenamiento: del total de datos, tomamos las filas aletorizadas que tienen datos en index
train <- Casas[index, ]
# Anteponiendo el "-" escogemos las filas en de la base que no están en index
test  <- Casas[-index, ]
ggplot(data=Casas)+ #Se define un gráfico con ggplot()
aes(x=Sale_Price)+ #Solo le ingresamos el eje "x" para un histograma
geom_histogram(col="black", fill="green", alpha = 0.2) # Se define la forma del gráfico. "col" pinta el contorno, "fill" el entorno y "alpha" entrega transparencia
ggplot(data=Casas)+
aes(x=log(Sale_Price))+
geom_histogram(col="black", fill="green", alpha=0.2)+
xlab("Log(Precio de venta)")+ #Etiqueta para el eje x
ylab("Frecuencia")+ #Etiqueta para el eje y
ggtitle("Distribución log(Precio de venta)")+ #Título del gráfico
theme(plot.title = element_text(hjust = 0.5)) #centra el título en el gráfico. Lo ajusta en la posición horizontal (hjust = 0.5)
#R no siempre interpreta bien la naturaleza de las variables: Roof_Style y Heating_QC son factores, pero están como character(string)
#en train
train$Roof_Style=as.factor(train$Roof_Style)
train$Heating_QC=as.factor(train$Heating_QC)
#en test
test$Roof_Style=as.factor(test$Roof_Style)
test$Heating_QC=as.factor(test$Heating_QC)
library(gridExtra) #Para unir gráficas
g1 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size=1, alpha=0.4) + #"size" aumenta el tamaño de los puntos, "alpha" da transparencia
geom_smooth(se=FALSE) + #Agregamos un ajuste no lineal sobre los puntos. "se" integra errores estándares
xlab("Año de construcción")
g2 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size = 1, alpha = .4) +
geom_smooth(method = "lm", se = FALSE) + #Agregamos un ajuste lineal sobre los puntos.
scale_y_log10() + # "scale_y_log10" transforma el eje "y" a logaritmo.
xlab("Año de construcción")
grid.arrange(g1, g2, nrow = 1) #une las  gráficas.
#R no siempre interpreta bien la naturaleza de las variables: Roof_Style y Heating_QC son factores, pero están como character(string)
#en train
train$Roof_Style=as.factor(train$Roof_Style)
train$Heating_QC=as.factor(train$Heating_QC)
#en test
test$Roof_Style=as.factor(test$Roof_Style)
test$Heating_QC=as.factor(test$Heating_QC)
ggplot(Casas) +
aes(x=Gr_Liv_Area, y=Sale_Price, col=Heating_QC)+ #Se agrega una dimensión de colores "col".
geom_point(size=1, alpha=0.4) +
geom_smooth(se=FALSE, method="lm") +
xlab("Espacio habitable")
ggplot(Casas) +
aes(x=Roof_Style, y=log(Sale_Price)) +
geom_boxplot(alpha=0.4, fill="black") #cambiamos el tipo de gráfico
train.lm <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC, #Fórmula
data = train, #Datos
method = "lm", #Algoritmo
trControl = trainControl(method = "cv", number = 10) #Method = cross validation, number=10 (k-fold)
)
test.lm  <- predict(train.lm , newdata=test) #Vector de datos predichos. Recibe una base de datos (newdata) y un modelo entrenado (train.lm)
error.lm <- test$Sale_Price-test.lm #Calcular los errores de predicción (dato real - dato estimado)
train.lm <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC, #Fórmula
data = train, #Datos
method = "lm", #Algoritmo
trControl = trainControl(method = "cv", number = 10) #Method = cross validation, number=10 (k-fold)
)
test.lm  <- predict(train.lm , newdata=test) #Vector de datos predichos. Recibe una base de datos (newdata) y un modelo entrenado (train.lm)
error.lm <- test$Sale_Price-test.lm #Calcular los errores de predicción (dato real - dato estimado)
#Ejecutar MARS (Multivariate adaptive regression spline)
train.mars <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train,
method="earth", #MARS
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"), #Pre-procesa datos. "center" resta el promedio de las variables, "scale" las divide por la desviación estandar. Esto ayuda para el tratamiento de outliers.
tuneLength = 5 #Indica que pruebe diferentes valores por default para el parámetro principal
)
print(train.mars)
ggplot(train.mars)
test.mars  <- predict(train.mars, newdata=test) #Vector de datos predichos
error.mars <- test$Sale_Price-test.mars #(dato real - dato estimado)
### Ejecutar KNN
train.knn <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="knn",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.knn)
ggplot(train.knn)
test.knn  <- predict(train.knn, newdata=test)
error.knn <- test$Sale_Price-test.knn
### Ejecutar CART (Classification and Regression Trees)
train.cart <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rpart2",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.cart)
ggplot(train.cart)
test.cart  <- predict(train.cart, newdata=test)
error.cart <- test$Sale_Price-test.cart
### Ejecutar Random Forest
train.randomf <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rf",
trControl = trainControl("cv", number=10),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.randomf)
ggplot(train.randomf)
test.randomf  <- predict(train.randomf, newdata=test)
error.randomf <- test$Sale_Price-test.randomf
sales.test <- data.frame(lm=test.lm, mars=unname(test.mars),  knn=test.knn,  cart=test.cart,  rf=test.randomf, sales=test$Sale_Price)
error.test <- data.frame(lm=error.lm, mars=unname(error.mars), knn=error.knn, cart=error.cart, rf=error.randomf)
summary(abs(error.test))
summary(error.test)
boxplot(abs(subset(error.test, select=-lm))); title(main="ML models", sub="Forecasting Absolute Errors")
boxplot(subset(error.test, select=-lm)); title(main="ML models", sub="Forecasting Errors")
Casas$Kitchen_Qual
Casas$Kitchen_Qual
which(colnames(Casas) %in% c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")
)
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
#Construya una base de datos reducida con "Casas <- Casas[,c(x1,x2,x3,...x10)]"
# x1,...,x10 son las posiciones de las variables en la base de datos Casas
View(Casas)
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas))
# Si no forma parte de los numéricos
if !(col %in% c("Garage_Area", "Gr_Liv_Area")){
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas))
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas)){}
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas)){
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas)){
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
class(Casas)
# Construcción de la sub-base
Casas <- Casas[,c("Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas)){
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
str(Casas)
# Construcción de la sub-base
Casas <- Casas[,c("Sale_Price", "Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())				 #Limpia todos los objetos creados en R
graphics.off()			 #Limpia los gráficos
options(digits = 3)  #Dígitos después del punto para observar (décimas, centésimas,...)
set.seed(12345)      #Fijar semilla de aleatoriedad
# setwd("C:/Users/color/Downloads/Marketing/LAB2_IN5602") #Fijar directorio de preferencia
library(readr)     #Para leer CSV
library(glmnet)    #Para ajustar modelo lineal
library(corrplot)  #Para realizar correlogramas
library(dplyr)     #Para manipulación de datos
library(ggplot2)   #Para gráficos
library(caret)     #Para validar y entrenar los modelos
library(knitr)
Casas <- read.csv("Casas.csv")
kable(Casas[1:5,1:9]) #kable crea una tabla head(Casas) sirve para visualiar la data
# Construimos un vector que esta formado por números de filas aleatoriamente
index <- sample(1:nrow(Casas), size= nrow(Casas)*0.7)
# Base de entrenamiento: del total de datos, tomamos las filas aletorizadas que tienen datos en index
train <- Casas[index, ]
# Anteponiendo el "-" escogemos las filas en de la base que no están en index
test  <- Casas[-index, ]
ggplot(data=Casas)+ #Se define un gráfico con ggplot()
aes(x=Sale_Price)+ #Solo le ingresamos el eje "x" para un histograma
geom_histogram(col="black", fill="green", alpha = 0.2) # Se define la forma del gráfico. "col" pinta el contorno, "fill" el entorno y "alpha" entrega transparencia
ggplot(data=Casas)+
aes(x=log(Sale_Price))+
geom_histogram(col="black", fill="green", alpha=0.2)+
xlab("Log(Precio de venta)")+ #Etiqueta para el eje x
ylab("Frecuencia")+ #Etiqueta para el eje y
ggtitle("Distribución log(Precio de venta)")+ #Título del gráfico
theme(plot.title = element_text(hjust = 0.5)) #centra el título en el gráfico. Lo ajusta en la posición horizontal (hjust = 0.5)
#R no siempre interpreta bien la naturaleza de las variables: Roof_Style y Heating_QC son factores, pero están como character(string)
#en train
train$Roof_Style=as.factor(train$Roof_Style)
train$Heating_QC=as.factor(train$Heating_QC)
#en test
test$Roof_Style=as.factor(test$Roof_Style)
test$Heating_QC=as.factor(test$Heating_QC)
library(gridExtra) #Para unir gráficas
g1 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size=1, alpha=0.4) + #"size" aumenta el tamaño de los puntos, "alpha" da transparencia
geom_smooth(se=FALSE) + #Agregamos un ajuste no lineal sobre los puntos. "se" integra errores estándares
xlab("Año de construcción")
g2 <- ggplot(Casas) +
aes(x=Year_Built, y=Sale_Price) +
geom_point(size = 1, alpha = .4) +
geom_smooth(method = "lm", se = FALSE) + #Agregamos un ajuste lineal sobre los puntos.
scale_y_log10() + # "scale_y_log10" transforma el eje "y" a logaritmo.
xlab("Año de construcción")
grid.arrange(g1, g2, nrow = 1) #une las  gráficas.
ggplot(Casas) +
aes(x=Gr_Liv_Area, y=Sale_Price, col=Heating_QC)+ #Se agrega una dimensión de colores "col".
geom_point(size=1, alpha=0.4) +
geom_smooth(se=FALSE, method="lm") +
xlab("Espacio habitable")
ggplot(Casas) +
aes(x=Roof_Style, y=log(Sale_Price)) +
geom_boxplot(alpha=0.4, fill="black") #cambiamos el tipo de gráfico
train.lm <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC, #Fórmula
data = train, #Datos
method = "lm", #Algoritmo
trControl = trainControl(method = "cv", number = 5) #Method = cross validation, number=10 (k-fold)
)
test.lm  <- predict(train.lm , newdata=test) #Vector de datos predichos. Recibe una base de datos (newdata) y un modelo entrenado (train.lm)
error.lm <- test$Sale_Price-test.lm #Calcular los errores de predicción (dato real - dato estimado)
#Ejecutar MARS (Multivariate adaptive regression spline)
train.mars <- train(form = Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train,
method="earth", #MARS
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"), #Pre-procesa datos. "center" resta el promedio de las variables, "scale" las divide por la desviación estandar. Esto ayuda para el tratamiento de outliers.
tuneLength = 5 #Indica que pruebe diferentes valores por default para el parámetro principal
)
print(train.mars)
ggplot(train.mars)
test.mars  <- predict(train.mars, newdata=test) #Vector de datos predichos
error.mars <- test$Sale_Price-test.mars #(dato real - dato estimado)
### Ejecutar KNN
train.knn <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="knn",
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.knn)
ggplot(train.knn)
test.knn  <- predict(train.knn, newdata=test)
error.knn <- test$Sale_Price-test.knn
### Ejecutar CART (Classification and Regression Trees)
train.cart <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rpart2",
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.cart)
ggplot(train.cart)
test.cart  <- predict(train.cart, newdata=test)
error.cart <- test$Sale_Price-test.cart
### Ejecutar Random Forest
train.randomf <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rf",
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.randomf)
ggplot(train.randomf)
test.randomf  <- predict(train.randomf, newdata=test)
error.randomf <- test$Sale_Price-test.randomf
sales.test <- data.frame(lm=test.lm, mars=unname(test.mars),  knn=test.knn,  cart=test.cart,  rf=test.randomf, sales=test$Sale_Price)
error.test <- data.frame(lm=error.lm, mars=unname(error.mars), knn=error.knn, cart=error.cart, rf=error.randomf)
summary(abs(error.test))
summary(error.test)
boxplot(abs(subset(error.test, select=-lm))); title(main="ML models", sub="Forecasting Absolute Errors")
boxplot(subset(error.test, select=-lm)); title(main="ML models", sub="Forecasting Errors")
# Construcción de la sub-base
Casas <- Casas[,c("Sale_Price", "Street", "Lot_Shape", "Exter_Cond", "Central_Air", "Garage_Area", "Kitchen_Qual", "Year_Built", "Gr_Liv_Area", "Roof_Style", "Heating_QC")]
# Para evitar problemas pasamos los atributos de tipo
# categóricos a factores
for (col in colnames(Casas)){
# Si no forma parte de los numéricos
if (!col %in% c("Garage_Area", "Gr_Liv_Area", "Year_Built")){
Casas[[col]] <- as.factor(Casas[[col]])
}
}
# Pasamos la variable dependiente (Sales Prices) a numerica
Casas$Sale_Price = as.numeric(Casas$Sale_Price)
# Tampoco tiene sentido el año numéricamente, pero son demasiados factores para analizarlo como tal. Por ende, vamos a transformar este atrbibuto a la distancia entre el último año en la base con la columna en cuestión, con lo que tendremos una diferencia de años que sí tiene sentido numéricamente para comparar
Casas$Year_Built <- max(Casas$Year_Built) - Casas$Year_Built
# Vemos las descripciones de los atributos
str(Casas)
# samples aleatorio
index <- sample(1:nrow(Casas), size= nrow(Casas)*0.7)
# entrenamiento 70%
train <- Casas[index, ]
# test 30%
test  <- Casas[-index, ]
#Corra 1 modelo de regresión lineal y 2 modelos de Machine Learning vistos en la clase con la base de datos reducida,  para predecir el precio de las casas.
train.lm <- train(form = Sale_Price ~ ., #Fórmula
data = train, #Datos
method = "lm", #Algoritmo
trControl = trainControl(method = "cv", number = 5) #Method = cross validation, number=10 (k-fold)
)
print(train.lm)
test.lm  <- predict(train.lm, newdata=test)
error.lm <- test$Sale_Price-test.lm
### Ejecutar KNN
train.knn <- train(Sale_Price ~ .,
data=train, method="knn",
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.knn)
ggplot(train.knn)
test.knn  <- predict(train.knn, newdata=test)
error.knn <- test$Sale_Price-test.knn
### Ejecutar Random Forest
train.randomf <- train(Sale_Price ~ Year_Built+Gr_Liv_Area+Roof_Style+Heating_QC,
data=train, method="rf",
trControl = trainControl("cv", number=5),
preProcess = c("center","scale"),
tuneLength = 5
)
print(train.randomf)
ggplot(train.randomf)
test.randomf  <- predict(train.randomf, newdata=test)
error.randomf <- test$Sale_Price-test.randomf
# Comparación de Modelos: Compare los modelos a través de métricas presentadas en el laboratorio.
# ¿Cuál es el mejor modelo predictivo?
sales.test <- data.frame(lm=test.lm, knn=test.knn,  rf=test.randomf, sales=test$Sale_Price)
error.test <- data.frame(lm=error.lm, knn=error.knn, rf=error.randomf)
summary(abs(error.test))
summary(error.test)
boxplot(abs(subset(error.test, select=-lm))); title(main="ML models", sub="Forecasting Absolute Errors")
boxplot(subset(error.test, select=-lm)); title(main="ML models", sub="Forecasting Errors")
# Comparación de Modelos: Compare los modelos a través de métricas presentadas en el laboratorio.
# ¿Cuál es el mejor modelo predictivo?
sales.test <- data.frame(lm=test.lm, knn=test.knn,  rf=test.randomf, sales=test$Sale_Price)
error.test <- data.frame(lm=error.lm, knn=error.knn, rf=error.randomf)
summary(abs(error.test))
summary(error.test)
boxplot(abs(subset(error.test))); title(main="ML models", sub="Forecasting Absolute Errors")
boxplot(subset(error.test)); title(main="ML models", sub="Forecasting Errors")
