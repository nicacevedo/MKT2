---
title: 'Lab 4: Otros modelos de clasificación - Otoño 2021'
author: 'Nicolás Acevedo, Constanza Bastías y Pablo Ubilla'
date: "23/6/2021"
output:
  html_document:
    df_print: paged
    theme: simplex
    highlight: tango
    toc: yes
encoding: UTF-8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning

Machine Learning es un conjunto de métodos que Automatizar la construcción de modelos analíticos y se basa en la idea de que los sistemas pueden aprender de los datos, identificar patrones y tomar decisiones con una mínima intervención humana.

Los problemas más típicos que resuelven los modelos de Machine Learning son 2:

1.  Regresiones: Predicción de una variable continua
2.  Clasificación: Predeción de una variable discreta

# Laboratorio {.tabset}

## El Dataset

Trabajaremos con una base de datos que contiene los resultados de un análisis químico de vinos cultivados en Italia. Tres diferentes tipos de vino están representados en las 178 muestras, con los resultados de 13 análisis químicos registrados para cada muestra. La variable 'Type' corresponde a la variable categórica.

Los datos no contienen valores perdidos y se componen de solo datos numéricos, con la variable objetivo de tres clases 'Type' para la clasificación.

A continuación un diccionario de las variables:

-   **Type** : Tipo de vino (factor)
-   **Alcohol** : Alcohol (numérica)
-   **Malic**: Ácido málico (numérica)
-   **Ash**: Ceniza (numérica)
-   **Alcalinity**: Alcalinidad de la ceniza (numérica)
-   **Magnesium**: Magnesio (numérica)
-   **Phenols**: Fenoles totales (numérica)
-   **Flavanoids**: Flavonoides (numérica)
-   **Nonflavanoids**: Fenoles no flavonoides (numérica)
-   **Proanthocyanins** : Proantocianinas (numérica)
-   **Color**: Intensidad del color (numérica)
-   **Hue**: Matiz (numérica)
-   **Dilution**: D280 / OD315 de vinos diluidos (numérica)
-   **Proline**: Prolina (numérica)

```{r someVar, echo=FALSE, results="hide",include=FALSE}
library(caret) #librería modelos de ML
library(dplyr) #librería para usar antijoint y crear test data
library(rattle) #librería que contiene la BD wine
library('ggplot2')
library('MLmetrics')
rm(list=ls())   # Limpiamos todos los objetos creados en R. la función ls() indica los nombres de todos los objetos y rm() los remueve.
graphics.off()  # Limpiamos los gráficos en el ambiente Plots (esquina inferior derecha)
set.seed(12345) #Fijamos una semilla de aleatoriedad. Se debe realizar si se toman muestras aleatorias o se generan datos aleatorios. 
#install.packages('rattle')

wine=wine #agregamos la data al workspace
wine$Type=as.numeric(wine$Type) #convertimos a numerica la columna de tipo de vino para posteriormente convertirla a character
```

Head del dataset:

```{r}
head(wine[sample(nrow(wine),6 ), ])
```

## EDA

Para usar modelos de ML para clasificar es necesario que las categorías estén como texto:

```{r, echo=FALSE}
#character variable Type
wine$Type[wine$Type== 1] <- 'Tipo1'
wine$Type[wine$Type== 2] <- 'Tipo2'
wine$Type[wine$Type== 3] <- 'Tipo3'
```

Vemos la distribución de los tipos de vino en el dataset:

```{r}
ggplot(wine, aes(Type, fill=Type)) + geom_bar(alpha=0.8) +
  labs(x="Tipo", y="Número de Vinos en categoría") +
  guides(fill=FALSE)+ggtitle("Frecuencia de tipo de vinos")
```

```{r}
ggplot(wine, aes(x=Type,y=Alcohol,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Grado Alcohólico")+ggtitle("Boxplot de niveles de alcohol por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Malic,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Malic")+ggtitle("Boxplot de Malic por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Ash,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Ash")+ggtitle("Boxplot de Ash por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(Alcalinity, fill=Type)) + 
  geom_histogram(binwidth=1,alpha=0.8) +
  labs(fill="Tipo", x="Grados de alcohol", y="Número de vinos")+ggtitle("Frecuencia de Alcalinidad por tipo de vino")
```

```{r}
ggplot(wine, aes(x=Type,y=Magnesium,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Magnesium")+ggtitle("Boxplot de Magnesium por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Phenols,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Phenols")+ggtitle("Boxplot de Phenols por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Flavanoids,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Flavanoids")+ggtitle("Boxplot de Flavanoids por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Nonflavanoids,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Flavanoids")+ggtitle("Boxplot de Flavanoids por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Proanthocyanins,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Flavanoids")+ggtitle("Boxplot de Flavanoids por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(Color, fill=Type)) + 
  geom_histogram(binwidth=1,alpha=0.8) +
  labs(fill="Tipo", x="Intensidad del color", y="Número de vinos")+ggtitle("Frecuencia de Color por tipo de vino")
```

```{r}
ggplot(wine, aes(x=Type,y=Hue,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Hue")+ggtitle("Boxplot de Hue por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Dilution,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Dilution")+ggtitle("Boxplot de Hue por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Proline,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Proline")+ggtitle("Boxplot de Proline por tipo de vino")+guides(fill=FALSE)
```

```{r}
ggplot(wine, aes(x=Type,y=Color,fill=Type)) + 
  geom_boxplot(alpha=0.8) +
  labs(fill="Tipo", x="Tipo de vino", y="Coloración")+ggtitle("Boxplot de intensidad de color por tipo de vino")+guides(fill=FALSE)
```

## KNN

### K-Nearest Neighbors

<img src="https://helloacm.com/wp-content/uploads/2016/03/2012-10-26-knn-concept.png" alt="KNN"/>

```{r}
trainwine=wine[sample(nrow(wine),100 ), ]
testwine=anti_join(wine, trainwine)

  
train.knn <- train( Type ~.,data=trainwine, method="knn",
                   trControl  = trainControl("cv",number=5,classProbs=TRUE, summaryFunction=multiClassSummary),
                   preProcess = c("center","scale"),
                   tuneLength = 6,
                   metric = 'logLoss')

print(train.knn)

ggplot(train.knn)

train.knn$results


test.knn<-predict(train.knn,newdata=testwine)
confusionMatrix(as.factor(test.knn),as.factor(testwine$Type))

Metrica=c('Precision','Recall','F1_Score', 'Accuracy')
Valores=c(Precision(testwine$Type,test.knn),Recall(testwine$Type,test.knn),F1_Score(testwine$Type,test.knn),Accuracy(testwine$Type,test.knn))
KNN=data.frame(Metrica,Valores)
#print(KNN)
head(KNN)
```

## Random Forest

### Random Forest

<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png" alt="RF "/>

```{r}
train.rf <- train( Type ~.,data=trainwine, method="rf",
                   trControl  = trainControl("cv",number=5,classProbs=TRUE, summaryFunction=multiClassSummary),
                   preProcess = c("center","scale"),
                   tuneLength = 6,
                   metric= 'logLoss')

print(train.rf)

ggplot(train.rf)

train.rf$results


test.rf<-predict(train.rf,newdata=testwine)
confusionMatrix(as.factor(test.rf),as.factor(testwine$Type))

Metrica=c('Precision','Recall','F1_Score', 'Accuracy')
Valores=c(Precision(testwine$Type,test.rf),Recall(testwine$Type,test.rf),F1_Score(testwine$Type,test.rf),Accuracy(testwine$Type,test.rf))
rf=data.frame(Metrica,Valores)
#print(rf)
head(rf)
```

## SVM

### Suppot Vector Machines

<img src="https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png" alt="RF "/>

```{r}
train.svm <- train( Type ~.,data=trainwine, method="svmLinear2",
                   trControl  = trainControl("cv",number=5,classProbs=TRUE, summaryFunction=multiClassSummary),
                   preProcess = c("center","scale"),
                   tuneLength = 6,
                   metric= 'logLoss')

print(train.svm)

ggplot(train.svm)

train.svm$results


test.svm<-predict(train.svm,newdata=testwine)
confusionMatrix(as.factor(test.svm),as.factor(testwine$Type))

Metrica=c('Precision','Recall','F1_Score', 'Accuracy')
Valores=c(Precision(testwine$Type,test.svm),Recall(testwine$Type,test.svm),F1_Score(testwine$Type,test.svm),Accuracy(testwine$Type,test.svm))
svm=data.frame(Metrica,Valores)
#print(svm)
head(svm)
```

## Métricas

### Métricas en modelos de clasificación

-   *Accuracy* Indica sobre el total de elementos cuantos son predichos correctamente

$$Accuracy=\frac{TruePositives+TrueNegatives}{TruePositives+FalsePositives+TrueNegatives+FalseNegatives}$$

-   *Precision* Indica respecto a los elementos predichos como positivos cuantos lo son efectivamente.

$$Precision=\frac{TruePositives}{TruePositives+FalsePositives}$$

-   *Recall - Sensibility* Mide respecto a todos los elementos que efectivamente son positivos cuantos fueron predichos correctamente. (Exhaustividad )

$$Recall=\frac{TruePositives}{TruePositives+FalseNegatives}$$

-   *F1-Score* Entrega un promedio ponderado entre precision y recall.

$$F1_{score}=\frac{2⋅Recall⋅Precision}{Recall+Presicion}$$

## Preguntas

##### Pregunta 1

```{r p1}
# Seleccione 3 variables del dataset para generar las predicciones y argumente por qué serían buenas variables para predecir el tipo de vino. Puede realizar un EDA o basarse en los gráficos ya incluidos.
# Determine 2 métricas para comparar los modelos, justifique su elección.
# Si se viera en la obligación de NO agregar ciertas variables del dataset, comente sobre cuál o cuáles serían apoyándose en un EDA o los gráficos ya incluídos (Mínimo una variable)
```

Observando el EDA, seleccionamos las siguientes variables:

-   Dilución: mide cuán diluido está un tipo de vino, y si observamos el EDA, podemos notar que hay alta variabilidad entre los tipos de vinos, por lo que nos podría ayudar a identificar mejor qué tipo de vino es al momento de correr un modelo.

-   Grado alcoholico: mide cuánto alcohol hay en una cierta medida del vino, y la seleccionamos por un argumento similar al anterior, pues hay variabilidad entre los tipos de vino (dan números diferentes entre ellos), lo que ayudaría a identificarlos mejor.

-   Intensidad de color: mide el grado en que la luz puede atravesar el vino (ve la opacidad), y tal como en las variables anteriores, esta métrica cambia bastante entre los tipos de vinos.

Las métricas que utilizaremos para comparar modelos son:

-   Accuracy: pues queremos ver qué tan bien funcionan nuestros modelos, y si son capaces de identificar bien el tipo de vino.

-   F1: Pues considera las métricas de Recall y Precision, lo que nos entrega una información más completa y comparable al pesar de igual manera la cantidad de falsos positivos y falsos negativos (a través de los ratios entregados por Recall y Precision), lo que nos entrega una métrica mucho más acorde a la clasificación cuando tenemos clases desbalanceadas.

Algunas variables que no incorporaremos, son "Hue" o el "Malic", pues observando el EDA, notamos que para el tipo de vino 1 y 2 son bastantes similares, por lo que no aportan demasiada infromación con respecto a la diferencia ente todas las clases, y, dado que tenemos otras variables que sí varian bastante entre clases, es mejor mantener estas y no agregar posible información que cause confusiones.

##### Pregunta 2

```{r p2}
# Corra 2 modelos de clasificación con las variables seleccionadas anteriormente para predecir el tipo de vino.
# Cálcule para cada modelo las metricas seleccionadas para el set de testeo, además muestre la matriz de confusión.
# Comente sobre los resultados, ¿Los modelos poseen capacidad de generalización? ¿se observa overfitting o underfitting?
```

```{r}
#Primero creamos un dataset con las variables escogidas
df <- wine[,c(1,2,11,13)]
head(df)
```

```{r}
# Creamos los sets de entrenamiento y testeo
train_df=df[sample(nrow(df),100 ), ]
test_df=anti_join(df, train_df)
```

Los modelos a utilizar son KNN y SVM:

KNN

```{r}
train.knn <- train( Type ~.,
                    data=train_df, 
                    method="knn",
                    trControl  = trainControl("cv",
                                              number=5,
                                              classProbs=TRUE, 
                                              summaryFunction=multiClassSummary
                                              ),
                   preProcess = c("center","scale"),
                   tuneLength = 6,
                   metric = 'logLoss'
                   )

print(train.knn)
ggplot(train.knn)
train.knn$results
```

```{r}
#Testeamos
test.knn<-predict(train.knn,newdata=test_df)
confusionMatrix(as.factor(test.knn),as.factor(test_df$Type))
```

```{r}
# Obtenemos las métricas
Metrica=c('F1_Score', 'Accuracy')
Valores=c(F1_Score(test_df$Type,test.knn),Accuracy(test_df$Type,test.knn))
KNN=data.frame(Metrica,Valores)
head(KNN)
```

SVM

```{r}
#Usamos los mismos sets de train y test

train.svm <- train( Type ~.,data=train_df, method="svmLinear2",
                   trControl  = trainControl("cv",number=5,classProbs=TRUE, summaryFunction=multiClassSummary),
                   preProcess = c("center","scale"),
                   tuneLength = 6,
                   metric= 'logLoss')

print(train.svm)
ggplot(train.svm)
train.svm$results
```

```{r}
test.svm<-predict(train.svm,newdata=testwine)
confusionMatrix(as.factor(test.svm),as.factor(testwine$Type))
```

```{r}
Metrica=c('F1_Score', 'Accuracy')
Valores=c(F1_Score(test_df$Type,test.svm),Accuracy(test_df$Type,test.svm))
svm=data.frame(Metrica,Valores)
head(svm)
```

Observando la matriz de confusión de cada modelo, podemos observar que en ambos casos el balanced accuracy es alto ( 0.94 el más bajo para KNN y 0.92 el más bajo para SVM), por lo que los modelos estarían prediciendo bien el tipo de vino de manera transversal (sin tener en cuenta el desbalance), presentando una cantidad relativamente baja de falsos negativos y positivos en ambos modelos, aunque con mejor rendimiento en KNN al presentar menos y en 1 sola clase (la 2), por lo que en KNN podemos decir que el Tipo 1 y 3 se clasifican muy bien, mientras que en el Tipo 2 podemos presentar algunos resultados bajos, mientras que en SVM tenemos falsos resultados en todas las clases. Justamente por esto es que es relevante el F1-Score, donde podemos ver que en el caso de KNN es de aproximadamente 0.94, mientras que que en SVM es cercano a 0.84, por lo que esta métrica da cuenta justamente de la situación que vimos en la matriz de confusión: KNN predice mucho mejor a las clases 1 y 3 por lo que al medir de manera justa cada clase vemos que esto aumenta el rendimiento de KNN en comparación a SVM, lo que el Accuracy muestra en menor cantidad.

Sobre la presencia de over/underfitting, podemos notar que los sets de entrenamiento y testeo fueron divididos en aproximadamente 56% de set de entrenamiento y 44% de set de testeo, por lo que, de partida, estamos testeando con casi la misma cantidad de observaciones que con las que entrenamos, lo que vuelve mucho más robusto nuestro testeo, y al observar las métricas de interés sobre el test de testeo que ya mencionamos en el párrafo anterior, podemos concluir que claramente no existe overfitting, ya que dan valores muy buenos en ambos modelos para este set "externo" al entrenamiento del modelo, sobretodo para KNN, por lo que no se puede concluir que no existe overfitting en nuestros modelos y son generalizables. El underfitting se produce cuando el modelo no es capaz siquiera de ajustarse bien al set de entrenamiento, lo que claramente implica que es casi imposible también que se ajuste bien al de testeo, por lo que al ver los resultados del set de testeo también debería bastar para decir que no existe underfitting, pero sólo para guiarnos explícitamente por la definición de este, vemos que en las métricas entregadas por el reporte del entrenamiento en ambos modelos, el mínimo valor de F1 promedio es 0.88, mientras que el Accuracy Balanceado siempre supera el 0.9, por lo que se ve lo que esperábamos, que no existe underfitting si miramos los resultados sobre el set de entrenamiento.

##### Pregunta 3

```{r p3}
# Compare los modelos a través de métricas presentadas en el laboratorio. ¿Con que modelo se quedaría? 
```

Ambos modelos funcionan bastante bien, considerando que tomamos un set de entrenamiento casi del mismo tamaño que el set de testeo para poner a prueba tanto los modelos como las variables y demostrar que las variables que se escogieron realmente eran capaces de explicar bastante de cada clase. Sin embargo, es necesario mencionar que el modelo KNN es el que presenta mejores resultados en ambas métricas, presentando mejor Accuracy y mejor F1-Score, por lo que claramente funciona mejor que SVM a la hora de clasificar en la cantidad de datos que tenemos, lo que tiene sentido, ya que este no depende demasiado de la cantidad de datos si se tiene suficiente variabilidad, ya que sólo necesita de unos pocos datos similares a la observación que se quiere clasificar para poder arrojar un resultado, mientras que en SVM se necesita generar un plano separador que depende de las observaciones más cercanas a este, por lo que no es suficiente con la variabilidad, también se necesitan suficiente valores extramos cercanos al gap entre clases además de esta, por lo que con pocos datos podrían no obtenerse tan buenos resultados tal y como pudimos ver en nuetros resultados. Por último, KNN es muy "barato" computacionalmente y arroja excelente métricas, por lo que sin ninguna duda nos quedaríamos con este modelo.

Podemos resumir las métricas en la siguiente tabla:

|          | KNN       | SVM       |
|----------|-----------|-----------|
| F1       | 0.9583333 | 0.8936170 |
| Accuracy | 0.9487179 | 0.7948718 |
